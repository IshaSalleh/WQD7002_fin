{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"WQD7002 Step 2 v2 description field cleaning.ipynb","provenance":[{"file_id":"1pUk_bPnY6BhntuhkJnvQkc2aM5JDbF-G","timestamp":1574532546265},{"file_id":"1g8UVJxgQoO5v6eX3Op5iRoKv7UtPAYxk","timestamp":1573969867767},{"file_id":"1-cOIyZUDcaqXSrIjyRqmv0AoPzIlg3CC","timestamp":1573649579890},{"file_id":"14XETtrb2JfE6bv7TUrHC4P-GORjGRni4","timestamp":1573647299768}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gLB3uC9VOY2U","colab_type":"text"},"source":["# CHAPTER 4\n","## WQD7002 (P2) \n","## WQD170031"]},{"cell_type":"markdown","metadata":{"id":"P6pHagFcOY2q","colab_type":"text"},"source":["## Steps thought of, will need to write them in for chapter 3 of the full report.\n","<br>1. data understanding - to identify fields important to the analysis (data accuracy etc.)\n","<br>2. text preprocessing - first remove common word (use nltk?); then perform 1 hot encoding (depending on how data need to be preprocessed to use the association rule library)???\n","<br>3. market basket analysis - association rule analysis - use pre-written libraries or copy from the coder who wrote his own algo for association rule analysis?\n","<br>4. evaluation by benchmarking to other metrics - see if high lift or support correspond to high number of lenders lending for the loan (or high amount raised in the crowdfunding) (or lesser time taken to raise the loan)"]},{"cell_type":"code","metadata":{"id":"2wKCMe_OOY26","colab_type":"code","colab":{}},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ezvl5PWis2uN","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"5SmhCvWqTshT","colab_type":"code","outputId":"84cd7b92-3786-4139-9a43-58e4e5777992","executionInfo":{"status":"ok","timestamp":1574850422386,"user_tz":-480,"elapsed":27157,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["#try see if the code above stays the same next session\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nkE56O_CTzaW","colab_type":"code","colab":{}},"source":["!ls '/content/drive/My Drive/kiva data/from build.kiva.org/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z7e4tmaCwfaP","colab_type":"code","colab":{}},"source":["my_folder = '/content/drive/My Drive/kiva data/from build.kiva.org/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ahZ4aIWugjW8","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import datetime"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kq1z4JsARLZ0","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","\n","df=pd.read_csv(my_folder+'loans_v2.csv')\n","\n","df[['POSTED_TIME','PLANNED_EXPIRATION_TIME','DISBURSE_TIME','RAISED_TIME','REPORT_DT']] = \\\n","df[['POSTED_TIME','PLANNED_EXPIRATION_TIME','DISBURSE_TIME','RAISED_TIME','REPORT_DT']].apply(pd.to_datetime)\n","\n","df[df.select_dtypes(include=['int']).columns]=df[df.select_dtypes(include=['int']).columns].apply(pd.to_numeric,downcast='unsigned')\n","\n","df[df.select_dtypes(include=['float']).columns]=df[df.select_dtypes(include=['float']).columns].apply(pd.to_numeric,downcast='float')\n","\n","df[['ORIGINAL_LANGUAGE','STATUS','ACTIVITY_NAME','SECTOR_NAME','COUNTRY_CODE','COUNTRY_NAME','CURRENCY_POLICY','CURRENCY','REPAYMENT_INTERVAL','DISTRIBUTION_MODEL']]=\\\n","df[['ORIGINAL_LANGUAGE','STATUS','ACTIVITY_NAME','SECTOR_NAME','COUNTRY_CODE','COUNTRY_NAME','CURRENCY_POLICY','CURRENCY','REPAYMENT_INTERVAL','DISTRIBUTION_MODEL']].astype('category')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VHUuYfwU8AYd","colab_type":"code","colab":{}},"source":["df.info()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P0caZ5vU7A3Q","colab_type":"code","colab":{}},"source":["import dask.dataframe as dd\n","df=dd.read_csv(my_folder+'loans_v2.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ujL2CHSlOY3m","colab_type":"text"},"source":["## Load dataset"]},{"cell_type":"code","metadata":{"id":"5e6g8OyJEHHk","colab_type":"code","colab":{}},"source":["#store the processes above as a python function\n","def data_overview(data):\n","  tmp=data.isnull().sum().reset_index(name='n_miss').rename({'index':'variable'},axis=1) #calculate number of missing values\n","  tmp2=data.notnull().sum().reset_index(name='n_notmiss').rename({'index':'variable'},axis=1) #calculate number of non-missing values, https://stackoverflow.com/questions/47044183/count-non-null-values-in-pandas\n","  tmp=pd.merge(tmp,tmp2,how='left',left_on='variable',right_on='variable') #join the tables so that can recycle variable name\n","  tmp['n']=tmp['n_miss']+tmp['n_notmiss']\n","  tmp2=data.nunique().reset_index(name='n_unique').rename({'index':'variable'},axis=1) #calculate number of unique values\n","  tmp=pd.merge(tmp,tmp2,how='left',left_on='variable',right_on='variable')\n","  tmp['miss_rate']=tmp['n_miss']/tmp['n']\n","  tmp['fill_rate']=tmp['n_notmiss']/tmp['n']\n","  tmp['distinct_rate']=tmp['n_unique']/tmp['n']\n","  tmp2=data.dtypes.reset_index(name='vartype').rename({'index':'variable'},axis=1) #get variable type\n","  tmp=pd.merge(tmp,tmp2,how='left',left_on='variable',right_on='variable')\n","  #tmp.to_excel(my_folder+'dataset_overview.xls')\n","  return tmp"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"zHJPE39qOY9B","colab_type":"code","colab":{}},"source":["df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-CWZNKAbDHOS","colab_type":"text"},"source":["### Generate additional fields for analysis purposes\n","\n","-might also be used in the results part depending on how i see the direction of the research report?\n","<br>-<DEL>borrower genders into number of males, number of females and number of borrowers</del>\n","<br><del>-borrower_pictured into number_of_borrowers_pictured (actually can drop this idea coz shown below same to number of borrowers)</del>\n","<br>-number of sentences, <del>number of words</del> in description and description_translated\n","-[don't feel like the number of sentences is achievable, coz dots were also used for name short forms.]\n","<br><del>-combine description to description_translated for english to increase fill rate, call description_new.\n","<br>-number of words in loan use\n","<br>-number of words in  activity name</del>\n"]},{"cell_type":"code","metadata":{"id":"wago3gQlDFZv","colab_type":"code","colab":{}},"source":["#check number of rows in the dataset\n","df['DESC_TRANSLATED_WORDCOUNT'] = df['DESCRIPTION_TRANSLATED'].str.split().str.len() #https://stackoverflow.com/questions/37483470/how-to-calculate-number-of-words-in-a-string-in-dataframe\n","#df['DESC_TRANSLATED_SENTENCE_COUNT'] = df['DESCRIPTION_TRANSLATED'].str.split().str.len()\n","#derive variable counting number of description to identify duplicates\n","df['DESC_WORDCOUNT'] = df['DESCRIPTION'].str.split().str.len()\n","#df['DESC_SENTENCE_COUNT'] = df['DESCRIPTION'].str.split().str.len()\n","#derive variable counting number of description to identify duplicates\n","\n","df['DESCRIPTION_NEW'] = np.where(df['ORIGINAL_LANGUAGE']=='English', df['DESCRIPTION'], df['DESCRIPTION_TRANSLATED'])   #combine description and description_translated\n","df['DESC_NEW_WORDCOUNT'] = df['DESCRIPTION_NEW'].str.split().str.len()\n","#df['DESC_NEW_SENTENCE_COUNT'] = df['DESCRIPTION_NEW'].str.split().str.len()\n","#derive variable to count number of description to identify duplicates\n","\n","df['LOAN_USE_WORDCOUNT'] = df['LOAN_USE'].str.split().str.len()\n","#derive variable to check number of sentences in loan use\n","df['AVY_NAME_WORDCOUNT'] = df['ACTIVITY_NAME'].str.split().str.len()\n","#derive variable to check number of sentences in activity name\n","\n","#variable to get number of females and males (borrowers)\n","\n","#df['DESC_WORDCOUNT'].value_counts(ascending=True).to_csv(my_folder+'desc_wordcount.csv')\n","#df['DESC_WORDCOUNT'].value_counts(ascending=True).reset_index(name='freq').freq.value_counts().\\\n","#reset_index().freq.describe(percentiles=[.01,.05,.1,.25,.5,.75,.9,.95,.99])#.to_csv(my_folder+'wordcount_freq.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"crjgVNrY7aM-","colab_type":"code","colab":{}},"source":["df['AVY_NAME_CLEAN_LM_WORDCOUNT'] = df['AVY_NAME_CLEAN_LEMMA'].str.split().str.len()\n","df['AVY_NAME_CLEAN_PS_WORDCOUNT'] = df['AVY_NAME_CLEAN_PS'].str.split().str.len()\n","\n","df['TAGS_CLEAN_LM_WORDCOUNT'] = df['TAGS_CLEAN_LEMMA'].str.split().str.len()\n","df['TAGS_CLEAN_PS_WORDCOUNT'] = df['TAGS_CLEAN_PS'].str.split().str.len()\n","\n","df['LOAN_USE_CLEAN_LM_WORDCOUNT'] = df['LOAN_USE_CLEAN_LEMMA'].str.split().str.len()\n","df['LOAN_USE_CLEAN_PS_WORDCOUNT'] = df['LOAN_USE_CLEAN_PS'].str.split().str.len()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FcECxweorHXW","colab_type":"code","colab":{}},"source":["#count number of repeated description\n","#https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html\n","tmp=df.groupby('DESCRIPTION_TRANSLATED').size().reset_index(name='N_DESC_TRANSLATED')\n","df=pd.merge(df,tmp,how='left',left_on='DESCRIPTION_TRANSLATED',right_on='DESCRIPTION_TRANSLATED')\n","\n","tmp=df.groupby('DESCRIPTION').size().reset_index(name='N_DESC')\n","df=pd.merge(df,tmp,how='left',left_on='DESCRIPTION',right_on='DESCRIPTION')\n","\n","tmp=df.groupby('DESCRIPTION_NEW').size().reset_index(name='N_DESC_NEW')\n","df=pd.merge(df,tmp,how='left',left_on='DESCRIPTION_NEW',right_on='DESCRIPTION_NEW')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rw2h6LKhtDeh","colab_type":"code","colab":{}},"source":["df['N_BORROWERS'] = df['BORROWER_GENDERS'].str.split().str.len()\n","df['N_BORROWERS2'] = df['BORROWER_PICTURED'].str.split().str.len() #created as cross-check against variable built above\n","df['GROUP_LOANS']=np.where(df['N_BORROWERS']>1,\"group\",\"individual\")\n","\n","df['N_TAGS'] = df['TAGS'].str.split().str.len()\n","\n","df['N_F_BORROWERS']=df['BORROWER_GENDERS'].str.lower().str.count(\"female\")\n","df['N_M_BORROWERS']=df['N_BORROWERS']-df['N_F_BORROWERS']\n","\n","df['HAVE_PICTURE']=np.where(df['IMAGE_ID'].isnull(),0,1)\n","df['HAVE_VIDEO']=np.where(df['VIDEO_ID'].isnull(),0,1)\n","df['N_BORROWERS_PICTURED']=df['BORROWER_PICTURED'].str.lower().str.count(\"true\")  #n borrowers pictured\n","#df['BORROWER_GENDERS'].head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AMhbrsBlCjAu","colab_type":"code","colab":{}},"source":["df['N_TAGS'] = df['TAGS'].str.split().str.len()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzwLoxSI0YgR","colab_type":"code","colab":{}},"source":["#to use as a way to countercheck success in fundraising activity on Kiva.org\n","#if -ve then unsuccessful and we should expect this to tally with STATUS='REFUNDED/EXPIRED'\n","#if value >=0 means it is successful\n","#at the same time can inspect of the value can exceed 0:\n","##would like to expect that the value will never exceed 0\n","##once the requested amount have been fulfilled then the funding for that particular loan closes\n","df['D_RAISED_LOANED']=df['FUNDED_AMOUNT']-df['LOAN_AMOUNT']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MAUIn8B3jbkO","colab_type":"text"},"source":["### generate additional time related fields\n","-understanding the time frame in the data downloaded\n","<br>-suspect that RAISED_TIME is time when a loan is fully funded by Kiva lenders on Kiva\n","<br>--if this is true, then the dates should always be later than POSTED_TIME, which is time when a loan is posted by FP for fundraising in Kiva.\n","<br><del>-compare POSTED_TIME and DISBURSED_TIME as a way to check which loans have been disbursed first from FP to borrower</del> coded \n","<br>-suspect that PLANNED_EXPIRATION_TIME is the time allocated/targeted to raise the fund\n","<br>--if the above is true then this value should be really small (not more than 3 months)\n","<br><del>-compare PLANNED_EXPIRATION_TIME and DISBURSED_TIME, and PLANNED_EXPIRATION_TIME and POSTED_TIME</del> coded \n","<br>--to verify whether PLANNED_EXPIRATION_TIME is same as LENDER_TERM or different\n","<BR>--if above is true then then LENDER_TERM should have a value closely associated with one of the calculated fields\n"]},{"cell_type":"code","metadata":{"id":"ciooVsHBsiu6","colab_type":"code","colab":{}},"source":["#library to be used\n","#http://www.rohitschauhan.com/index.php/2018/06/13/python-date-manipulation-for-sas-users/\n","import datetime as dt\n","import numpy as np\n","import re\n","import string"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WVjmuT67rlfM","colab_type":"code","outputId":"3005cb07-aa64-4853-eb4a-789be85aa6d6","executionInfo":{"status":"ok","timestamp":1573820964003,"user_tz":-480,"elapsed":196755,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["#calculate the time fields\n","#end_date_series.dt.to_period('M') - start_date_series.dt.to_period('M') \n","\n","#suspect that RAISED_TIME is time when a loan is fully funded by Kiva lenders on Kiva\n","##if this is true, then the dates should always be later than POSTED_TIME,\n","##which is time when a loan is posted by FP for fundraising in Kiva.\n","df['N_MTH_POSTED_RAISED']=df['RAISED_TIME'].dt.to_period('M') - df['POSTED_TIME'].dt.to_period('M')\n","\n","#compare POSTED_TIME and DISBURSED_TIME as a way to check which loans have been disbursed first from FP to borrower \n","df['N_MTH_DISBURSED_POSTED']=df['POSTED_TIME'].dt.to_period('M')-df['DISBURSE_TIME'].dt.to_period('M') #should have more +ve values here\n","\n","#compare PLANNED_EXPIRATION_TIME and DISBURSED_TIME, and PLANNED_EXPIRATION_TIME and POSTED_TIME \n","##to verify whether PLANNED_EXPIRATION_TIME is same as LENDER_TERM or different\n","df['N_MTH_DISBURSED_EXPIRE']=df['PLANNED_EXPIRATION_TIME'].dt.to_period('M')-df['DISBURSE_TIME'].dt.to_period('M')\n","df['N_MTH_POSTED_EXPIRE']=df['PLANNED_EXPIRATION_TIME'].dt.to_period('M')-df['POSTED_TIME'].dt.to_period('M')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/arrays/datetimes.py:1269: UserWarning:\n","\n","Converting to PeriodArray/Index representation will drop timezone information.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wO22AKVvCZ3i","colab_type":"code","colab":{}},"source":["#https://stackoverflow.com/questions/43876281/extract-all-numeric-characters-from-a-pandas-series-all-groups\n","#x['clean'] = x.phone.str.replace(r'\\D+', '')\n","\n","#df['N_MTH_POSTED_RAISED']=df['N_MTH_POSTED_RAISED'].str.replace(r'\\D+','')\n","#df['N_MTH_DISBURSED_POSTED']=df['N_MTH_DISBURSED_POSTED'].str.replace(r'\\D+','')\n","#df['N_MTH_DISBURSED_EXPIRE']=df['N_MTH_DISBURSED_EXPIRE'].str.replace(r'\\D+','')\n","#df['N_MTH_POSTED_EXPIRE']=df['N_MTH_POSTED_EXPIRE'].str.replace(r'\\D+','')\n","\n","df[['N_MTH_POSTED_RAISED','N_MTH_DISBURSED_POSTED','N_MTH_DISBURSED_EXPIRE','N_MTH_POSTED_EXPIRE']]=\\\n","df[['N_MTH_POSTED_RAISED','N_MTH_DISBURSED_POSTED','N_MTH_DISBURSED_EXPIRE','N_MTH_POSTED_EXPIRE']].apply(pd.to_numeric)\n","\n","df[['N_MTH_POSTED_RAISED','N_MTH_DISBURSED_POSTED','N_MTH_DISBURSED_EXPIRE','N_MTH_POSTED_EXPIRE']].head(10)\n","df[['N_MTH_POSTED_RAISED','N_MTH_DISBURSED_POSTED','N_MTH_DISBURSED_EXPIRE','N_MTH_POSTED_EXPIRE']].dtypes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CX0jNmfBy9Pm","colab_type":"code","colab":{}},"source":["df['REPORT_DT'] = df['POSTED_TIME'] + pd.offsets.MonthEnd()\n","df[['REPORT_DT']].head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KAMG2x3kBy3-","colab_type":"code","colab":{}},"source":["#checkpoint\n","df.to_csv(my_folder+'loans_v2.csv',index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VQ5jDRapqoaw","colab_type":"code","colab":{}},"source":["#https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sas.html\n","#split the date information by year, month  \n","#try to consider time of day, week of month, week of year, day (Sun-Fri)\n","#do this based on posted_time\n","df['POSTED_YEAR'] = df['POSTED_TIME'].dt.year\n","df[['POSTED_YEAR']].head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H5o5DuOhv27I","colab_type":"code","outputId":"87ea2e1f-a353-4fd2-d7fc-4c91d9c2c6f4","executionInfo":{"status":"ok","timestamp":1574306061393,"user_tz":-480,"elapsed":16937,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["#https://stackoverflow.com/questions/51287504/remove-the-time-from-datetime-datetime-in-pandas-column\n","#http://www.datasciencemadesimple.com/difference-two-timestamps-seconds-minutes-hours-pandas-python-2/\n","#from datetime import date\n","\n","#d0 = date(2008, 8, 18)\n","#d1 = date(2008, 9, 26)\n","#delta = d1 - d0\n","#print(delta.days)\n","\n","df['N_DAYS_POSTED_RAISED']=df['RAISED_TIME'].dt.date - df['POSTED_TIME'].dt.date\n","\n","#compare POSTED_TIME and DISBURSED_TIME as a way to check which loans have been disbursed first from FP to borrower \n","#df['N_DAYS_DISBURSED_POSTED']=df['POSTED_TIME'].dt.date - df['DISBURSE_TIME'].dt.date #should have more +ve values here\n","df['N_HRS_POSTED_RAISED']=((df['RAISED_TIME']-df['POSTED_TIME'])/np.timedelta64(1,'h')).round()\n","\n","df[['N_HRS_POSTED_RAISED','N_DAYS_POSTED_RAISED']].head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>N_HRS_POSTED_RAISED</th>\n","      <th>N_DAYS_POSTED_RAISED</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>13.0</td>\n","      <td>0 days</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>215.0</td>\n","      <td>9 days</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>25.0</td>\n","      <td>1 days</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>15.0</td>\n","      <td>0 days</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>118.0</td>\n","      <td>5 days</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   N_HRS_POSTED_RAISED N_DAYS_POSTED_RAISED\n","0                 13.0               0 days\n","1                215.0               9 days\n","2                 25.0               1 days\n","3                 15.0               0 days\n","4                118.0               5 days"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"OHg2mVYb75Pb","colab_type":"code","outputId":"4c80e6fd-6020-48c1-dfd5-ddf89bf565f4","executionInfo":{"status":"error","timestamp":1573970355623,"user_tz":-480,"elapsed":745,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":217}},"source":["#https://stackoverflow.com/questions/46786211/counting-the-frequency-of-words-in-a-pandas-data-frame\n","#df.ACTIVITY_NAME.str.split(expand=True).stack().value_counts().to_excel(my_folder+'avy_name_wordfreq.xls')\n","#df.LOAN_USE.str.split(expand=True).stack().value_counts().to_excel(my_folder+'loan_use_wordfreq.xlsx')\n","df.DESCRIPTION.str.split(expand=True).stack().value_counts().to_excel(my_folder+'desc_wordfreq.xlsx')\n","df.DESCRIPTION_NEW.str.split(expand=True).stack().value_counts().to_excel(my_folder+'desc_new_wordfreq.xlsx')\n","df.DESCRIPTION_TRANSLATED.str.split(expand=True).stack().value_counts().to_excel(my_folder+'desc_trans_wordfreq.xlsx')"],"execution_count":0,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-2e17ac3a28d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDESCRIPTION\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'desc_wordfreq.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDESCRIPTION_NEW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'desc_new_wordfreq.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDESCRIPTION_TRANSLATED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'desc_trans_wordfreq.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: split() got an unexpected keyword argument 'expand'"]}]},{"cell_type":"code","metadata":{"id":"B06FvIRVsDoA","colab_type":"code","colab":{}},"source":["#df['DESC_WORDCOUNT'].value_counts(ascending=True).to_csv(my_folder+'desc_wordcount.csv')\n","df['DESC_WORDCOUNT'].value_counts(ascending=True).reset_index(name='freq').freq.value_counts().\\\n","#reset_index().freq.describe(percentiles=[.01,.05,.1,.25,.5,.75,.9,.95,.99])#.to_csv(my_folder+'wordcount_freq.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wO2PYbDOboq6","colab_type":"text"},"source":["## Charts to understand the datasets"]},{"cell_type":"markdown","metadata":{"id":"ImfWiDdQII3M","colab_type":"text"},"source":["### functions defined for plotting charts"]},{"cell_type":"code","metadata":{"id":"-CKp901Vt5bA","colab_type":"code","colab":{}},"source":["#create a function to generate the plotly horizontal bar chart for frequencies of categorical variables\n","#https://stackoverflow.com/questions/35692781/python-plotting-percentage-in-seaborn-bar-plot\n"," \n","import plotly.graph_objects as go\n","import seaborn as sns\n","import plotly.express as px\n","\n","def interactive_freqchart_h(data,variable):\n","   \n","   #generate frequency dataset\n","   tmp=data[variable].value_counts(normalize=True).\\\n","   reset_index(name='freq').rename(mapper={'index':variable},axis=1)#.\\\n","   #pipe((sns.barplot, \"data\"), x='freq', y='ORIGINAL_LANGUAGE', palette='ch:.25')\n","   \n","   tmp['freq']=tmp['freq']*100\n","   #sns.barplot(y='ORIGINAL_LANGUAGE',x='freq',data=tmp,order=tmp['ORIGINAL_LANGUAGE'])\n","\n","   fig = go.Figure(go.Bar(\n","       x=tmp['freq'],\n","       y=tmp[variable],\n","       orientation='h'))\n","   fig.update_layout(\n","       title=\"Distribution of \" + variable,\n","       xaxis_title=\"Frequency (in %)\",\n","       yaxis_title=variable)\n","   fig.show()\n","   #fig.show(config={'editable': True,'scrollZoom': True,'displayModeBar': True})\n","   #'modeBarButtonsToRemove': ['toggleSpikelines','hoverCompareCartesian']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"43lrf_YDwZSg","colab_type":"code","colab":{}},"source":["import plotly.graph_objects as go\n","\n","def interactive_freqchart_v(data,variable):\n","   \n","   #generate frequency dataset\n","   tmp=data[variable].value_counts(normalize=True).\\\n","   reset_index(name='freq').rename(mapper={'index':variable},axis=1)\n","   \n","   tmp['freq']=tmp['freq']*100\n","\n","   fig = go.Figure(go.Bar(\n","       x=tmp[variable],\n","       y=tmp['freq']))\n","   fig.update_layout(\n","       title=\"Distribution of \" + variable,\n","       xaxis_title=variable,\n","       yaxis_title=\"Frequency (in %)\")\n","   fig.show()\n","   #fig.show(config={'editable': True,'scrollZoom': True,'displayModeBar': True})\n","   #'modeBarButtonsToRemove': ['toggleSpikelines','hoverCompareCartesian']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4q7dCUe_Pc3","colab_type":"code","colab":{}},"source":["#plan to use boxplots later, https://plot.ly/python/box-plots/\n","import plotly.express as px\n","tips = px.data.tips()\n","fig = px.box(tips, x=\"time\", y=\"total_bill\")\n","fig.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lsRV-7GJOY-q","colab_type":"text"},"source":["## Step 2: text preprocessing\n","(Step 1 data understanding is covered inside another ipynb)\n","<br>It was very time consuming to pre-process the description field. try to process based on loan_use or activity_name first."]},{"cell_type":"code","metadata":{"id":"dzqv2uTwOY-u","colab_type":"code","colab":{}},"source":["#df=pd.read_csv('E:/New folder/Google Drive/isha.salleh - backup and sync/kiva data/from build.kiva.org/loans_v2.csv')\n","df=pd.read_csv('/content/drive/My Drive/kiva data/from build.kiva.org/loans_v3.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_IW-rgRaAUIa","colab_type":"code","outputId":"080d0233-f431-49fb-b8ed-2733dd216ceb","executionInfo":{"status":"ok","timestamp":1573552823694,"user_tz":-480,"elapsed":835,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["df['DESC_WORDCOUNT'].describe(percentiles=[.01,.05,.1,.25,.5,.75,.9,.95,.99])\n","#consider adding a chart to visualize the calculated DESC_WORDCOUNT"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    1.229155e+06\n","mean     1.246997e+02\n","std      5.355588e+01\n","min      1.000000e+00\n","1%       4.900000e+01\n","5%       6.400000e+01\n","10%      7.100000e+01\n","25%      8.700000e+01\n","50%      1.120000e+02\n","75%      1.510000e+02\n","90%      1.950000e+02\n","95%      2.260000e+02\n","99%      2.990000e+02\n","max      1.521000e+03\n","Name: DESC_WORDCOUNT, dtype: float64"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"ndTyFVl8FEfD","colab_type":"text"},"source":["Word count analysis. \n","\n","<br>Have max 1521 words used in a single description of a loan. \n","<br>May want to consider charting the frequency of DESC_WORDCOUNT to show how it looks like.\n","<br>(later add in observation from wordcount_freq.csv)\n","<br>most word counts seem to have value of 1"]},{"cell_type":"code","metadata":{"id":"iarS5eBs5kl2","colab_type":"code","outputId":"e6737e58-aa68-46a1-f365-f69b70ad89b4","executionInfo":{"status":"ok","timestamp":1574050695536,"user_tz":-480,"elapsed":36,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#extract first 5 rows to test whether algorithm works or not. \n","df2=df[~df.DESCRIPTION_TRANSLATED.isnull()].head(5).copy() #https://stackoverflow.com/questions/39475566/python-pandas-filter-out-records-with-null-or-empty-string-for-a-given-field\n","df2.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 61)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"GuGttQYKOY_a","colab_type":"code","colab":{}},"source":["#URL 1: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n","#URL 2: https://www.geeksforgeeks.org/python-stemming-words-with-nltk/\n","#URL 3: https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n","#URL 4: https://blog.ekbana.com/pre-processing-text-in-python-ad13ea544dae\n","import re       #removal of special symbols (Manura's code) and numbers\n","import string   #used in URL3 for removing punctuation symbols\n","import nltk\n","from nltk import pos_tag\n","from nltk.corpus import stopwords, wordnet\n","from nltk.stem.porter import PorterStemmer  #what is porterstemmer used for - URL 2 seem to indicate stemmer used to identify words like \"playing\" etc.\n","from nltk.tokenize import word_tokenize     #from URL #1 looks like this converts each words separated by space, and symbols into individual terms. \n","from nltk.stem import WordNetLemmatizer \n","#from nltk.tokenize import LineTokenizer\n","#may be can just use this instead of using one-hot encoding"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OuvQ5TwOY_v","colab_type":"code","outputId":"6b1c0e85-aec2-4ba0-ee1a-c16938313a58","executionInfo":{"status":"ok","timestamp":1574850453763,"user_tz":-480,"elapsed":2040,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["#had a problem with the stopwords just now....\n","nltk.download('stopwords')\n","nltk.download('punkt')  #to use word_tokenize\n","nltk.download('wordnet')  #to use WordNetLemmatizer\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6},{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6},{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"8Yru0DKwk-9G","colab_type":"code","colab":{}},"source":["#https://www.nltk.org/book/\n","from nltk.corpus import wordnet as wn\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk import word_tokenize, pos_tag\n","from collections import defaultdict\n","tag_map = defaultdict(lambda : wn.NOUN)\n","tag_map['J'] = wn.ADJ\n","tag_map['V'] = wn.VERB\n","tag_map['R'] = wn.ADV\n","tag_map['N'] = wn.NOUN"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vboOtHLshHWe","colab_type":"code","outputId":"6f13bfe6-219b-46cb-93a9-a6ddbf6e323e","executionInfo":{"status":"ok","timestamp":1574366129953,"user_tz":-480,"elapsed":1646,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["tag_map"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["defaultdict(<function __main__.<lambda>>,\n","            {'J': 'a', 'N': 'n', 'R': 'r', 'V': 'v'})"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ld8SQAuNOY_6","colab_type":"code","outputId":"2f70c4a1-6a23-4caa-a7a7-2bde6fd72995","executionInfo":{"status":"ok","timestamp":1573588701549,"user_tz":-480,"elapsed":1455,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["#viewing stopwords \n","len(stopwords.words('english'))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["179"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"VBfMPAPqOY_m","colab_type":"code","colab":{}},"source":["#https://www.geeksforgeeks.org/python-nltk-nltk-tokenize-conditionalfreqdist/\n","#try to have a better understanding of words to be removed by generating the frequency distribution of the terms\n","#apart from stemmer may want to consider using lemmatizer: https://www.geeksforgeeks.org/python-lemmatization-with-nltk/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-KR4inZOZAO","colab_type":"code","outputId":"0809589f-c50a-4c0d-9ce4-d3d635a27d74","executionInfo":{"status":"ok","timestamp":1574314134043,"user_tz":-480,"elapsed":1546,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":360}},"source":["#removal of unnecessary characters from the field\n","#may need to create additional corpus of common words based on borrower name to remove their name from the description column.\n","#add removal of symbols (comma, quote symbols from the sentence)\n","whos"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Variable                  Type             Data/Info\n","----------------------------------------------------\n","InteractiveShell          MetaHasTraits    <class 'IPython.core.inte<...>eshell.InteractiveShell'>\n","d0                        date             2008-08-18\n","d1                        date             2008-09-26\n","date                      type             <class 'datetime.date'>\n","delta                     timedelta        39 days, 0:00:00\n","df                        DataFrame                 LOAN_ID  ... N_H<...>682790 rows x 74 columns]\n","drive                     module           <module 'google.colab.dri<...>s/google/colab/drive.py'>\n","dt                        module           <module 'datetime' from '<...>b/python3.6/datetime.py'>\n","go                        module           <module 'plotly.graph_obj<...>plotly/graph_objects.py'>\n","interactive_freqchart_h   function         <function interactive_fre<...>hart_h at 0x7f35740fd840>\n","interactive_freqchart_v   function         <function interactive_fre<...>hart_v at 0x7f35d2ade268>\n","my_folder                 str              /content/drive/My Drive/k<...>data/from build.kiva.org/\n","np                        module           <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n","pd                        module           <module 'pandas' from '/u<...>ages/pandas/__init__.py'>\n","px                        module           <module 'plotly.express' <...>tly/express/__init__.py'>\n","re                        module           <module 're' from '/usr/lib/python3.6/re.py'>\n","sns                       module           <module 'seaborn' from '/<...>ges/seaborn/__init__.py'>\n","string                    module           <module 'string' from '/u<...>lib/python3.6/string.py'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"moUcD6XmGH9w","colab_type":"text"},"source":["### text cleaning function"]},{"cell_type":"code","metadata":{"id":"VmZLe-3t295I","colab_type":"code","colab":{}},"source":["#things to do to remove stopwords - too slow, what can be done to make it faster\n","#other concents that may not have been incorporated:\n","##1.URL 4 - contraction - expand words like \"ain't\" to \"am not\"\n","##2.URL 4 - spell check\n","##3.URL 2,3 & 4 - lemmatization and stemming\n","\n","#https://thispointer.com/pandas-6-different-ways-to-iterate-over-rows-in-a-dataframe-update-while-iterating-row-by-row/\n","import datetime\n","\n","def text_cleaning_lemma(data,varin,varout):\n","  desc_clean=[] #create a list to store cleaned up description field\n","  start_time=datetime.datetime.now()\n","  \n","  for row in data.itertuples():\n","    dictRow = row._asdict()\n","    if pd.isnull(dictRow[varin]):\n","      desc_clean.append(' ')\n","    else:\n","  #if description is nan then return empty space to desc_clean, otherwise proceed\n","      tmp=dictRow[varin].lower()  #1. extract the desription translated into separate words stored in a list\n","      tmp=tmp.strip('\\\\r\\\\n')   #https://stackoverflow.com/questions/45383938/cant-delete-r-n-from-a-string/45383951\n","      tmp=tmp.translate(str.maketrans(\"\", \"\",string.punctuation))  #2. remove symbols from URL 3, improved with https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n","      tmp = re.sub('rnrn', ' ', tmp) #some '\\\\r\\\\n' cant be removed earlier, so use https://stackoverflow.com/questions/3939361/remove-specific-characters-from-a-string-in-python\n","      tmp=re.sub(r'\\d+','',tmp)   #2a. remove numbers (check whether necessary or not)\n","      tmp=re.sub('<[^<]+?>','',tmp)  #2c. remove HTML tags, based on URL 4\n","    #3. check whether the terms inside are common words or not, if not, store in a variable separated by space/commas that will be appended back to the df?\n","    #want to change sequence: start with lemmatization, followed by stemmer, then autocorrect.\n","      lemmatizer=WordNetLemmatizer()\n","      tmp=pos_tag(word_tokenize(tmp))\n","      #tmp=[lemmatizer.lemmatize(word,pos='v') for word in tmp if not word in set(stopwords.words('english'))]\n","      tmp=[lemmatizer.lemmatize(word,pos=tag_map[tag[0]]) for word,tag in tmp if not word in set(stopwords.words('english'))]\n","      tmp=' '.join(tmp)\n","      #print(tmp)\n","      desc_clean.append(tmp)  #4. store in a list\n","\n","  data[varout]=desc_clean #5. append the list to df as a new df column\n","  #print('done in', datetime.datetime.now()-start_time,\"!\")\n","  return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"txN6S5nDE3xK","colab_type":"code","colab":{}},"source":["def text_cleaning_ps(data,varin,varout):\n","  start_time=datetime.datetime.now()\n","  desc_clean=[] #create a list to store cleaned up description field\n","\n","  for row in data.itertuples():\n","    dictRow = row._asdict()\n","    if pd.isnull(dictRow[varin]):\n","      desc_clean.append(' ')\n","    else:\n","  #if description is nan then return empty space to desc_clean, otherwise proceed\n","      tmp=dictRow[varin].lower()  #1. extract the desription translated into separate words stored in a list\n","      tmp=tmp.strip('\\\\r\\\\n')   #https://stackoverflow.com/questions/45383938/cant-delete-r-n-from-a-string/45383951\n","      tmp=tmp.translate(str.maketrans(\"\", \"\",string.punctuation))  #2. remove symbols from URL 3, improved with https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n","      tmp = re.sub('rnrn', ' ', tmp) #some '\\\\r\\\\n' cant be removed earlier, so use https://stackoverflow.com/questions/3939361/remove-specific-characters-from-a-string-in-python\n","      tmp=re.sub(r'\\d+','',tmp)   #2a. remove numbers (check whether necessary or not)\n","      tmp=re.sub('<[^<]+?>','',tmp)  #2c. remove HTML tags, based on URL 4\n","    #3. check whether the terms inside are common words or not, if not, store in a variable separated by space/commas that will be appended back to the df?\n","    #want to change sequence: start with lemmatization, followed by stemmer, then autocorrect.\n","     #try to skip stemming first?\n","      tmp=word_tokenize(tmp)\n","      ps=PorterStemmer()\n","      tmp=[ps.stem(word) for word in tmp if not word in set(stopwords.words('english'))]\n","      #3a. create own list of common words to further clean the field - names, geographical locations?\n","      tmp=' '.join(tmp)\n","      #print(tmp)\n","      desc_clean.append(tmp)  #4. store in a list\n","\n","  data[varout]=desc_clean #5. append the list to df as a new df column\n","  #print('done in', datetime.datetime.now()-start_time,\"!\")\n","  return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mb_3xK7HfDDm","colab_type":"code","colab":{}},"source":["def custom_stopword_code(data,wordvar,freqvar,n=1):\n","  custom_stopword=[words for words in data[data[freqvar]<=n][wordvar]]\n","  #[words for words in data[wordvar] if data[freqvar]<=n]\n","  return custom_stopword"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F9DFK5_Ydkp_","colab_type":"code","colab":{}},"source":["def text_cleaning_custom(data,varin,varout,vareval,stopword_data,freqvar,wordvar, n=1):\n","  start_time=datetime.datetime.now()\n","  desc_clean=[] #create a list to store cleaned up description field\n","  custom_stopword=[words for words in stopword_data[stopword_data[freqvar]<=n][wordvar]]\n","  custom_stopword.extend(['br','year','old','also','like','kiva','nwtf']) #only used for DESC_NEW field cleaning\n","  for row in data.itertuples():\n","    dictRow = row._asdict()\n","    if pd.isnull(dictRow[varin]):\n","      desc_clean.append(' ')\n","    else:\n","  #if description is nan then return empty space to desc_clean, otherwise proceed\n","      #tmp=word_tokenize(dictRow[varin].str)\n","      tmp=dictRow[varin].split()\n","      tmp=[word for word in tmp if not word in custom_stopword]\n","      tmp=[re.sub(r'\\d+','',word) for word in tmp]\n","      tmp=[re.sub('<[^<]+?>','',word) for word in tmp]\n","      tmp=[word.translate(str.maketrans(\"\", \"\",string.punctuation)) for word in tmp]\n","      #3a. create own list of common words to further clean the field - names, geographical locations?\n","      tmp=' '.join(tmp)\n","      #print(tmp)\n","      desc_clean.append(tmp)  #4. store in a list\n","\n","  data[varout]=desc_clean #5. append the list to df as a new df column\n","  time_taken = datetime.datetime.now() - start_time\n","  return data ,time_taken"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tAYkJPK4iYsU","colab_type":"text"},"source":["### start cleanup"]},{"cell_type":"code","metadata":{"id":"-HsqyfpQGquh","colab_type":"code","colab":{}},"source":["#full swing clean up, clean up DESCRIPTION_NEW field only first\n","\n","#df=text_cleaning_lemma(data=df,varin='LOAN_USE',varout='LOAN_USE_CLEAN_LEMMA')\n","#df=text_cleaning_lemma(data=df,varin='LOAN_USE',varout='LOAN_USE_CLEAN_PS')\n","#df=text_cleaning_ps(data=df,varin='ACTIVITY_NAME',varout='AVY_NAME_CLEAN_LEMMA')\n","#df=text_cleaning_ps(data=df,varin='ACTIVITY_NAME',varout='AVY_NAME_CLEAN_PS')\n","#df.to_csv(my_folder+'loans_v2.csv')\n","\n","#df_year=df[(df['POSTED_YEAR']==2018)].copy()\n","#df_year=text_cleaning_lemma(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_LEMMA')\n","#df_year=text_cleaning_ps(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_PS')\n","#df_year.to_csv(my_folder+'loans_2018.csv', index=False)\n","\n","#df_year=df[(df['POSTED_YEAR']==2017)].copy()\n","#df_year=text_cleaning_lemma(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_LEMMA')\n","#df_year=text_cleaning_ps(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_PS')\n","#df_year.to_csv(my_folder+'loans_2017.csv', index=False)\n","\n","#df_year=df[(df['POSTED_YEAR']==2016)].copy()\n","#df_year=text_cleaning_lemma(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_LEMMA')\n","#df_year=text_cleaning_ps(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_PS')\n","#df_year.to_csv(my_folder+'loans_2016.csv', index=False)\n","\n","#df_year=df[(df['POSTED_YEAR']==2015)].copy()\n","#df_year=text_cleaning_lemma(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_LEMMA')\n","#df_year=text_cleaning_ps(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_PS')\n","#df_year.to_csv(my_folder+'loans_2015.csv', index=False)\n","\n","df_year=df[(df['POSTED_YEAR']==2014)].copy()\n","df_year=text_cleaning_lemma(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_LEMMA')\n","df_year=text_cleaning_ps(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_PS')\n","df_year.to_csv(my_folder+'loans_2014.csv', index=False)\n","\n","#df_year=df[(df['POSTED_YEAR']<=2008) | (df['POSTED_YEAR']>=2019)].copy()\n","#df_year=text_cleaning_lemma(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_LEMMA')\n","#df_year=text_cleaning_ps(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_PS')\n","#df_year.to_csv(my_folder+'loans_othyr.csv', index=False)\n","\n","#df_year=df[(df['POSTED_YEAR']==2009)].copy()\n","#df_year=text_cleaning_lemma(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_LEMMA')\n","#df_year=text_cleaning_ps(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_PS')\n","#df_year.to_csv(my_folder+'loans_2009.csv', index=False)\n","\n","#df_year=df[(df['POSTED_YEAR']==2010)].copy()\n","#df_year=text_cleaning_lemma(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_LEMMA')\n","#df_year=text_cleaning_ps(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_PS')\n","#df_year.to_csv(my_folder+'loans_2010.csv', index=False)\n","\n","#df_year=df[(df['POSTED_YEAR']==2011)].copy()\n","#df_year=text_cleaning_lemma(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_LEMMA')\n","#df_year=text_cleaning_ps(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_PS')\n","#df_year.to_csv(my_folder+'loans_2011.csv', index=False)\n","\n","#df_year=df[(df['POSTED_YEAR']==2012)].copy()\n","#df_year=text_cleaning_lemma(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_LEMMA')\n","#df_year=text_cleaning_ps(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_PS')\n","#df_year.to_csv(my_folder+'loans_2012.csv', index=False)\n","\n","#df_year=df[(df['POSTED_YEAR']==2013)].copy()\n","#df_year=text_cleaning_lemma(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_LEMMA')\n","#df_year=text_cleaning_ps(data=df_year,varin='DESCRIPTION_NEW',varout='DESC_NEW_CLEAN_PS')\n","#df_year.to_csv(my_folder+'loans_2013.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JEArHhDntaR7","colab_type":"text"},"source":["### further text cleaning, reduce words with very low frequency based on the wordfreq table generated.\n","try with the cleaned up loan use field and reduce bit by bit to see "]},{"cell_type":"markdown","metadata":{"id":"Xskias_WYcFh","colab_type":"text"},"source":["#### loan use field clean up"]},{"cell_type":"code","metadata":{"id":"NRtnrYU2don-","colab_type":"code","colab":{}},"source":["df2017=pd.read_csv(my_folder+'loans_2017_v2.csv')\n","#df2018=pd.read_csv(my_folder+'loans_2018.csv')\n","#loanuse_wordfreq=pd.read_excel(my_folder+'loan_use_wordfreq.xlsx')\n","df2017.columns"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8HdOcLSoZBTa","colab_type":"code","outputId":"d3c83758-41e4-44ad-bbde-99a1db848f44","executionInfo":{"status":"ok","timestamp":1574733709833,"user_tz":-480,"elapsed":1312,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#filter out missing tags, description_new, loan_use, activity_name and fundraising cases.\n","#df2018.shape\n","#df2018 = df2018.dropna(axis=0, how='any', subset=['DESCRIPTION_NEW','LOAN_USE','ACTIVITY_NAME','TAGS'])\n","#df2018.shape \n","\n","#drop status fundraising\n","df2018['STATUS'].value_counts()\n","df2018 = df2018[~(df2018['STATUS']==\"fundRaising\")]\n","df2018['STATUS'].value_counts()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(229631, 77)"]},"metadata":{"tags":[]},"execution_count":14},{"output_type":"execute_result","data":{"text/plain":["(187922, 77)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"ouH6pIoqcaA4","colab_type":"code","outputId":"30b9c11d-3e73-4854-9436-8983ae7b3510","executionInfo":{"status":"ok","timestamp":1574734043834,"user_tz":-480,"elapsed":1726,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["#filter out missing tags, description_new, loan_use, activity_name and fundraising cases.\n","df2017.shape\n","df2017 = df2017.dropna(axis=0, how='any', subset=['DESCRIPTION_NEW','LOAN_USE','ACTIVITY_NAME','TAGS'])\n","df2017.shape \n","\n","#drop status fundraising\n","df2017['STATUS'].value_counts()\n","df2017 = df2017[~(df2017['STATUS']==\"fundRaising\")]\n","df2017['STATUS'].value_counts()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(225476, 79)"]},"metadata":{"tags":[]},"execution_count":19},{"output_type":"execute_result","data":{"text/plain":["(196075, 79)"]},"metadata":{"tags":[]},"execution_count":19},{"output_type":"execute_result","data":{"text/plain":["funded     186998\n","expired      9077\n","Name: STATUS, dtype: int64"]},"metadata":{"tags":[]},"execution_count":19},{"output_type":"execute_result","data":{"text/plain":["funded     186998\n","expired      9077\n","Name: STATUS, dtype: int64"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"bVCO0OIug8ZL","colab_type":"code","outputId":"2fd416bd-f8f2-4554-8f4a-b150c496e91f","executionInfo":{"status":"error","timestamp":1574799348406,"user_tz":-480,"elapsed":1537,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":182}},"source":["loanuse_wordfreq=loanuse_wordfreq.rename(columns={'Unnamed: 0':'word',0:'freq'})\n","loanuse_wordfreq.columns"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-e7c9564251a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloanuse_wordfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloanuse_wordfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'freq'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloanuse_wordfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'loanuse_wordfreq' is not defined"]}]},{"cell_type":"code","metadata":{"id":"I-JC89gyh0PV","colab_type":"code","outputId":"7cd44e68-d9c5-4a68-bc58-2226af03b28d","executionInfo":{"status":"ok","timestamp":1574743806121,"user_tz":-480,"elapsed":598,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":137}},"source":["#data ,time_taken = text_cleaning_custom(data,varin,varout,vareval,stopword_data,freqvar,wordvar, n=1)\n","\n","df2018, running_time = \\\n","text_cleaning_custom(data = df2018,varin = 'LOAN_USE_CLEAN_LEMMA',varout = 'LOAN_USE_LM2',vareval = 'LOAN_USE',\\\n","                     stopword_data = loanuse_wordfreq,freqvar = 'freq',wordvar = 'word', n=1)\n","print(running_time)\n","df2018['LOAN_USE_LM2'].head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0:11:49.816937\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0        tto pay tuition fee child report school month\n","1    tto buy farm input seed quality fertilizer hel...\n","2                           start build house land buy\n","3                         build sanitary toilet family\n","4                                             pay land\n","Name: LOAN_USE_LM2, dtype: object"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"ahDO13bkUCl1","colab_type":"code","colab":{}},"source":["df2018.columns\n","df2018=df2018.rename({'LOAN_USE_LM3':'LOAN_USE_LM3_OLD'}, axis=1)\n","df2018.columns"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"95ql2iiUAx-3","colab_type":"code","outputId":"ce77c61a-3093-4ec1-dc30-eee91dddfc75","executionInfo":{"status":"ok","timestamp":1574748783310,"user_tz":-480,"elapsed":305356,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":137}},"source":["#df2018=df2018.drop(columns=['LOAN_USE_LM3'], axis=1)  #drop LM3 if still cant establish crosstab\n","#df2018.columns\n","df2018, running_time = \\\n","text_cleaning_custom(data = df2018,varin = 'LOAN_USE_LM2',varout = 'LOAN_USE_LM3_V2',vareval = 'LOAN_USE',\\\n","                     stopword_data = new_cleaning_2018,freqvar = 'freq',wordvar = 'loanuse_term', n=15)\n","print(running_time)\n","df2018['LOAN_USE_LM3_V2'].head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0:05:03.435347\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0        tto pay tuition fee child report school month\n","1    tto buy farm input seed quality fertilizer hel...\n","2                           start build house land buy\n","3                         build sanitary toilet family\n","4                                             pay land\n","Name: LOAN_USE_LM3_V2, dtype: object"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"a3mwUyzyUvny","colab_type":"code","outputId":"a95cd9a0-596d-4bc4-d570-e6a45fe67b8e","executionInfo":{"status":"ok","timestamp":1574749077263,"user_tz":-480,"elapsed":293823,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":137}},"source":["df2018, running_time = \\\n","text_cleaning_custom(data = df2018,varin = 'LOAN_USE_LM3_OLD',varout = 'LOAN_USE_LM3',vareval = 'LOAN_USE',\\\n","                     stopword_data = new_cleaning_2018,freqvar = 'freq',wordvar = 'loanuse_term', n=15)\n","print(running_time)\n","df2018['LOAN_USE_LM3'].head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0:04:52.431415\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0               tto pay tuition fee child school month\n","1    tto buy farm input seed quality fertilizer hel...\n","2                           start build house land buy\n","3                         build sanitary toilet family\n","4                                             pay land\n","Name: LOAN_USE_LM3, dtype: object"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"_QctqUVfYq1q","colab_type":"text"},"source":["#### desc_new cleanup\n"]},{"cell_type":"code","metadata":{"id":"XNKqMM1PUgdC","colab_type":"code","colab":{}},"source":["df2017=pd.read_csv(my_folder+'loans_2017_v3.csv')\n","df2018=pd.read_csv(my_folder+'loans_2018_v3.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EqL-0V5ZZBPv","colab_type":"code","colab":{}},"source":["df2017['DESC_NEW_CLEAN_LEMMA'].str.split(expand=True).stack().value_counts().\\\n","reset_index(name='freq').rename(columns={'index':'desc_new_term'}).\\\n","to_excel(my_folder+'2017_desc_new_lm_wordfreq.xlsx')\n","\n","df2018['DESC_NEW_CLEAN_LEMMA'].str.split(expand=True).stack().value_counts().\\\n","reset_index(name='freq').rename(columns={'index':'desc_new_term'}).\\\n","to_excel(my_folder+'2018_desc_new_lm_wordfreq.xlsx')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbrclMTOpOSS","colab_type":"code","colab":{}},"source":["new_cleaning=pd.read_excel(my_folder+'2017_desc_new_lm_wordfreq.xlsx')\n","new_cleaning_2018=pd.read_excel(my_folder+'2018_desc_new_lm_wordfreq.xlsx')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T5kJyFkiaUrl","colab_type":"code","colab":{}},"source":["#try to see what happens if try to clean up further \n","#may need to ditch this and propose as4/tgFH1VihPo8xNxve-0YANth0_Rstgs_4-Db9lNLnmWBiIBlicFQHGh0 refinement in future work\n","#remove desc_new_term with wordfreq<300\n","#limit to <3000 columns, which is still cumbersome to eliminate association rules afterwards \n","df2017, running_time = \\\n","text_cleaning_custom(data = df2017,varin = 'DESC_NEW_CLEAN_LEMMA',varout = 'DESC_NEW_LM2',vareval = 'DESCRIPTION_NEW',\\\n","                     stopword_data = new_cleaning,freqvar = 'freq',wordvar = 'desc_new_term', n=300)\n","print(running_time)\n","\n","df2017['DESC_NEW_LM2'].str.split(expand=True).stack().value_counts().\\\n","reset_index(name='freq').rename(columns={'index':'desc_new_term'}).shape\n","\n","df2017['DESC_NEW_LM2'].head()\n","\n","df2017.to_csv(my_folder+'loans_2017_v4.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BbWflm_kadqX","colab_type":"code","colab":{}},"source":["#may need to perform extra cleaning up to also remove terms used in 2017?\n","#if based on 2018_desc_new_wordfreq should elimiate wordfreq <=500\n","df2018, running_time = \\\n","text_cleaning_custom(data = df2018,varin = 'DESC_NEW_CLEAN_LEMMA',varout = 'DESC_NEW_TMP',vareval = 'DESCRIPTION_NEW',\\\n","                     stopword_data = new_cleaning,freqvar = 'freq',wordvar = 'desc_new_term', n=300)\n","print(running_time)\n","\n","df2018['DESC_NEW_TMP'].str.split(expand=True).stack().value_counts().\\\n","reset_index(name='freq').rename(columns={'index':'desc_new_term'}).shape\n","\n","df2018, running_time = \\\n","text_cleaning_custom(data = df2018,varin = 'DESC_NEW_TMP',varout = 'DESC_NEW_LM2',vareval = 'DESCRIPTION_NEW',\\\n","                     stopword_data = new_cleaning_2018,freqvar = 'freq',wordvar = 'desc_new_term', n=300)\n","print(running_time)\n","\n","df2018['DESC_NEW_LM2'].str.split(expand=True).stack().value_counts().\\\n","reset_index(name='freq').rename(columns={'index':'desc_new_term'}).shape\n","\n","df2018['DESC_NEW_TMP'].head()\n","\n","df2018.to_csv(my_folder+'loans_2018_v4.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sxJQa2VoFEKQ","colab_type":"text"},"source":["### check frequency of terms to check for further cleaning that will be needed"]},{"cell_type":"code","metadata":{"id":"AwZu-cL_ANJ0","colab_type":"code","outputId":"1cdef830-0b09-4803-8501-80ed7c8bb58f","executionInfo":{"status":"ok","timestamp":1574558572028,"user_tz":-480,"elapsed":1052,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["loanuse_wordfreq.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(100338, 2)"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"18elfU7SlRxF","colab_type":"code","colab":{}},"source":["new_cleaning=pd.read_excel(my_folder+'loan_use_lm2_wordfreq.xlsx')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9AIcD9Te5zO","colab_type":"code","colab":{}},"source":["#data[variable]\n","new_cleaning=df2017['LOAN_USE_LM2'].str.split(expand=True).stack().value_counts().\\\n","reset_index(name='Freq').rename(columns={'index':'loanuse_term'})#.shape\n","#\\.to_excel(my_folder+'desc_wordfreq.xlsx')\n","new_cleaning['percent']=new_cleaning['Freq']/new_cleaning['Freq'].sum()\n","new_cleaning.head(5)\n","new_cleaning.shape\n","#new_cleaning.to_excel(my_folder+'loan_use_lm2_wordfreq.xlsx')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YwrD7jS8DLUh","colab_type":"code","outputId":"08db49f9-6742-412b-aeda-e08e05dc632b","executionInfo":{"status":"ok","timestamp":1574567884061,"user_tz":-480,"elapsed":4753,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["df2017['LOAN_USE_LM3'].str.split(expand=True).stack().value_counts().\\\n","reset_index(name='freq').rename(columns={'index':'loanuse_term'}).shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2694, 2)"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"vQCao4yQvF-O","colab_type":"code","outputId":"9f4d8f1c-1430-4466-98a5-f1c1d4979583","executionInfo":{"status":"ok","timestamp":1574749300126,"user_tz":-480,"elapsed":8104,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#new_cleaning_2018=\n","df2018['LOAN_USE_LM3_OLD'].str.split(expand=True).stack().value_counts().\\\n","reset_index(name='freq').rename(columns={'index':'loanuse_term'}).shape#.\\\n","#to_excel(my_folder+'2018_loanuse_lm2_wordfreq.xlsx')\n","df2018['LOAN_USE_LM3'].str.split(expand=True).stack().value_counts().\\\n","reset_index(name='freq').rename(columns={'index':'loanuse_term'}).shape\n","df2018['LOAN_USE_LM3_V2'].str.split(expand=True).stack().value_counts().\\\n","reset_index(name='freq').rename(columns={'index':'loanuse_term'}).shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6938, 2)"]},"metadata":{"tags":[]},"execution_count":61},{"output_type":"execute_result","data":{"text/plain":["(2394, 2)"]},"metadata":{"tags":[]},"execution_count":61},{"output_type":"execute_result","data":{"text/plain":["(2739, 2)"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"nY-O8-ENT0JB","colab_type":"code","colab":{}},"source":["whos"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7EnL5CVf52lz","colab_type":"code","outputId":"0ec95d61-285b-4354-96e4-9afd13ba8981","executionInfo":{"status":"ok","timestamp":1574749401733,"user_tz":-480,"elapsed":21941,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":480}},"source":["#df2017.to_csv(my_folder+'loans_2017_v3.csv', index=False)\n","#df2018=df2018.drop(columns=[['Unnamed: 0','words']],axis=1)\n","df2018.columns\n","df2018.to_csv(my_folder+'loans_2018_v3.csv', index=False)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['LOAN_ID', 'LOAN_NAME', 'ORIGINAL_LANGUAGE', 'DESCRIPTION',\n","       'DESCRIPTION_TRANSLATED', 'FUNDED_AMOUNT', 'LOAN_AMOUNT', 'STATUS',\n","       'IMAGE_ID', 'VIDEO_ID', 'ACTIVITY_NAME', 'SECTOR_NAME', 'LOAN_USE',\n","       'COUNTRY_CODE', 'COUNTRY_NAME', 'TOWN_NAME', 'CURRENCY_POLICY',\n","       'CURRENCY_EXCHANGE_COVERAGE_RATE', 'CURRENCY', 'PARTNER_ID',\n","       'POSTED_TIME', 'PLANNED_EXPIRATION_TIME', 'DISBURSE_TIME',\n","       'RAISED_TIME', 'LENDER_TERM', 'NUM_LENDERS_TOTAL',\n","       'NUM_JOURNAL_ENTRIES', 'NUM_BULK_ENTRIES', 'TAGS', 'BORROWER_NAMES',\n","       'BORROWER_GENDERS', 'BORROWER_PICTURED', 'REPAYMENT_INTERVAL',\n","       'DISTRIBUTION_MODEL', 'DESC_TRANSLATED_WORDCOUNT', 'DESC_WORDCOUNT',\n","       'DESCRIPTION_NEW', 'DESC_NEW_WORDCOUNT', 'LOAN_USE_WORDCOUNT',\n","       'AVY_NAME_WORDCOUNT', 'N_DESC_TRANSLATED', 'N_DESC', 'N_DESC_NEW',\n","       'N_BORROWERS', 'N_BORROWERS2', 'GROUP_LOANS', 'N_F_BORROWERS',\n","       'N_M_BORROWERS', 'HAVE_PICTURE', 'HAVE_VIDEO', 'N_BORROWERS_PICTURED',\n","       'N_MTH_POSTED_RAISED', 'N_MTH_DISBURSED_POSTED',\n","       'N_MTH_DISBURSED_EXPIRE', 'N_MTH_POSTED_EXPIRE', 'N_TAGS',\n","       'D_RAISED_LOANED', 'TAGS_CLEAN_LEMMA', 'TAGS_CLEAN_PS',\n","       'AVY_NAME_CLEAN_PS', 'LOAN_USE_CLEAN_PS', 'AVY_NAME_CLEAN_LEMMA',\n","       'LOAN_USE_CLEAN_LEMMA', 'AVY_NAME_CLEAN_LM_WORDCOUNT',\n","       'AVY_NAME_CLEAN_PS_WORDCOUNT', 'TAGS_CLEAN_LM_WORDCOUNT',\n","       'TAGS_CLEAN_PS_WORDCOUNT', 'LOAN_USE_CLEAN_LM_WORDCOUNT',\n","       'LOAN_USE_CLEAN_PS_WORDCOUNT', 'REPORT_DT', 'POSTED_YEAR',\n","       'N_DAYS_POSTED_RAISED', 'N_HRS_POSTED_RAISED', 'DESC_NEW_CLEAN_LEMMA',\n","       'DESC_NEW_CLEAN_PS', 'DESC_NEW_CLEAN_LM_WORDCOUNT',\n","       'DESC_NEW_CLEAN_PS_WORDCOUNT', 'LOAN_USE_LM2', 'LOAN_USE_LM3_OLD',\n","       'LOAN_USE_LM3_V2', 'LOAN_USE_LM3'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"markdown","metadata":{"id":"o1eie0CU4mBs","colab_type":"text"},"source":["### test whether can generate the crosstab or not "]},{"cell_type":"code","metadata":{"id":"Uu7nVRX-4klZ","colab_type":"code","colab":{}},"source":["df2017['words'] = df2017.LOAN_USE_LM3.str.strip().str.split('[\\W_]+')\n","df2017.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WncCn9cY3-81","colab_type":"code","outputId":"505de14b-908f-4fc4-a839-ed70c5a58376","executionInfo":{"status":"ok","timestamp":1574567947094,"user_tz":-480,"elapsed":62976,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["#import datetime\n","start_time=datetime.datetime.now()\n","\n","rows = list()\n","for row in df2017[['LOAN_ID', 'words']].iterrows():\n","    r = row[1]\n","    for word in r.words:\n","        rows.append((r.LOAN_ID, word))\n","\n","words = pd.DataFrame(rows, columns=['LOAN_ID', 'words'])\n","print(\"time taken:\",datetime.datetime.now()-start_time)\n","words['qty']=1\n","words.shape\n","words.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["time taken: 0:01:01.584605\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(1514745, 3)"]},"metadata":{"tags":[]},"execution_count":34},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>LOAN_ID</th>\n","      <th>words</th>\n","      <th>qty</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1401164</td>\n","      <td>purchase</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1401164</td>\n","      <td>pig</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1401164</td>\n","      <td>stock</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1401164</td>\n","      <td>vegetable</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1401164</td>\n","      <td>sale</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   LOAN_ID      words  qty\n","0  1401164   purchase    1\n","1  1401164        pig    1\n","2  1401164      stock    1\n","3  1401164  vegetable    1\n","4  1401164       sale    1"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"EAns1chC5Mpw","colab_type":"code","outputId":"d5c18c3f-6be9-4d40-bcf8-e73e11b02375","executionInfo":{"status":"ok","timestamp":1574568029622,"user_tz":-480,"elapsed":145433,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":301}},"source":["basket_sets=pd.crosstab(words['LOAN_ID'],words['words'])\n","#basket_sets=pd.crosstab(words['words'],words['LOAN_ID'])\n","basket_sets.shape\n","basket_sets.head(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(225476, 2699)"]},"metadata":{"tags":[]},"execution_count":35},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>words</th>\n","      <th></th>\n","      <th>a</th>\n","      <th>abaca</th>\n","      <th>ability</th>\n","      <th>able</th>\n","      <th>abroad</th>\n","      <th>abundant</th>\n","      <th>academic</th>\n","      <th>access</th>\n","      <th>accessory</th>\n","      <th>accommodate</th>\n","      <th>account</th>\n","      <th>accounting</th>\n","      <th>ace</th>\n","      <th>acetone</th>\n","      <th>achieve</th>\n","      <th>acquire</th>\n","      <th>acquisition</th>\n","      <th>acre</th>\n","      <th>acrylic</th>\n","      <th>activity</th>\n","      <th>adapt</th>\n","      <th>adapter</th>\n","      <th>adaptor</th>\n","      <th>add</th>\n","      <th>addition</th>\n","      <th>additional</th>\n","      <th>adequate</th>\n","      <th>adhesive</th>\n","      <th>adjustment</th>\n","      <th>administration</th>\n","      <th>adult</th>\n","      <th>advance</th>\n","      <th>advantage</th>\n","      <th>advertise</th>\n","      <th>advertisement</th>\n","      <th>advertising</th>\n","      <th>afford</th>\n","      <th>affordable</th>\n","      <th>africa</th>\n","      <th>...</th>\n","      <th>wood</th>\n","      <th>wooden</th>\n","      <th>wool</th>\n","      <th>woolen</th>\n","      <th>work</th>\n","      <th>worker</th>\n","      <th>workforce</th>\n","      <th>working</th>\n","      <th>workingroom</th>\n","      <th>workplace</th>\n","      <th>workroom</th>\n","      <th>workshop</th>\n","      <th>world</th>\n","      <th>would</th>\n","      <th>woven</th>\n","      <th>wrap</th>\n","      <th>wraparound</th>\n","      <th>wrapper</th>\n","      <th>wrench</th>\n","      <th>yam</th>\n","      <th>yard</th>\n","      <th>yarn</th>\n","      <th>yea</th>\n","      <th>year</th>\n","      <th>yearend</th>\n","      <th>yearâ</th>\n","      <th>yeast</th>\n","      <th>yellow</th>\n","      <th>yield</th>\n","      <th>yogurt</th>\n","      <th>young</th>\n","      <th>youth</th>\n","      <th>yuca</th>\n","      <th>yucca</th>\n","      <th>zinc</th>\n","      <th>zip</th>\n","      <th>zipper</th>\n","      <th>zoona</th>\n","      <th>zoonabranded</th>\n","      <th>â</th>\n","    </tr>\n","    <tr>\n","      <th>LOAN_ID</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1213974</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1213975</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1213976</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1213977</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1213978</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 2699 columns</p>\n","</div>"],"text/plain":["words       a  abaca  ability  able  ...  zip  zipper  zoona  zoonabranded  â\n","LOAN_ID                              ...                                     \n","1213974  0  0      0        0     0  ...    0       0      0             0  0\n","1213975  0  0      0        0     0  ...    0       0      0             0  0\n","1213976  0  0      0        0     0  ...    0       0      0             0  0\n","1213977  0  0      0        0     0  ...    0       0      0             0  0\n","1213978  0  0      0        0     0  ...    0       0      0             0  0\n","\n","[5 rows x 2699 columns]"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"JxGMOYE75Pio","colab_type":"code","colab":{}},"source":["for col in basket_sets.columns:\n","  if basket_sets[col].max()>1:\n","    print(col,\":\",basket_sets[col].max())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XkXzC--Mn8EX","colab_type":"code","colab":{}},"source":["def encode_units(x):\n","    if x <= 0:\n","        return 0\n","    if x >= 1:\n","        return 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F6R42a4ln_1V","colab_type":"code","colab":{}},"source":["basket_sets = basket_sets.applymap(encode_units)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U96Pato0oJgp","colab_type":"code","outputId":"a6fd52d9-c0b1-4899-bcb0-8b4202e1b713","executionInfo":{"status":"ok","timestamp":1574569224207,"user_tz":-480,"elapsed":5592,"user":{"displayName":"Nur Aishah Alia Mohd Sallehhuddin","photoUrl":"","userId":"03609661708769584950"}},"colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["basket_sets.head(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>words</th>\n","      <th></th>\n","      <th>a</th>\n","      <th>abaca</th>\n","      <th>ability</th>\n","      <th>able</th>\n","      <th>abroad</th>\n","      <th>abundant</th>\n","      <th>academic</th>\n","      <th>access</th>\n","      <th>accessory</th>\n","      <th>accommodate</th>\n","      <th>account</th>\n","      <th>accounting</th>\n","      <th>ace</th>\n","      <th>acetone</th>\n","      <th>achieve</th>\n","      <th>acquire</th>\n","      <th>acquisition</th>\n","      <th>acre</th>\n","      <th>acrylic</th>\n","      <th>activity</th>\n","      <th>adapt</th>\n","      <th>adapter</th>\n","      <th>adaptor</th>\n","      <th>add</th>\n","      <th>addition</th>\n","      <th>additional</th>\n","      <th>adequate</th>\n","      <th>adhesive</th>\n","      <th>adjustment</th>\n","      <th>administration</th>\n","      <th>adult</th>\n","      <th>advance</th>\n","      <th>advantage</th>\n","      <th>advertise</th>\n","      <th>advertisement</th>\n","      <th>advertising</th>\n","      <th>afford</th>\n","      <th>affordable</th>\n","      <th>africa</th>\n","      <th>...</th>\n","      <th>wood</th>\n","      <th>wooden</th>\n","      <th>wool</th>\n","      <th>woolen</th>\n","      <th>work</th>\n","      <th>worker</th>\n","      <th>workforce</th>\n","      <th>working</th>\n","      <th>workingroom</th>\n","      <th>workplace</th>\n","      <th>workroom</th>\n","      <th>workshop</th>\n","      <th>world</th>\n","      <th>would</th>\n","      <th>woven</th>\n","      <th>wrap</th>\n","      <th>wraparound</th>\n","      <th>wrapper</th>\n","      <th>wrench</th>\n","      <th>yam</th>\n","      <th>yard</th>\n","      <th>yarn</th>\n","      <th>yea</th>\n","      <th>year</th>\n","      <th>yearend</th>\n","      <th>yearâ</th>\n","      <th>yeast</th>\n","      <th>yellow</th>\n","      <th>yield</th>\n","      <th>yogurt</th>\n","      <th>young</th>\n","      <th>youth</th>\n","      <th>yuca</th>\n","      <th>yucca</th>\n","      <th>zinc</th>\n","      <th>zip</th>\n","      <th>zipper</th>\n","      <th>zoona</th>\n","      <th>zoonabranded</th>\n","      <th>â</th>\n","    </tr>\n","    <tr>\n","      <th>LOAN_ID</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1213974</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1213975</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1213976</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1213977</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1213978</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 2699 columns</p>\n","</div>"],"text/plain":["words       a  abaca  ability  able  ...  zip  zipper  zoona  zoonabranded  â\n","LOAN_ID                              ...                                     \n","1213974  0  0      0        0     0  ...    0       0      0             0  0\n","1213975  0  0      0        0     0  ...    0       0      0             0  0\n","1213976  0  0      0        0     0  ...    0       0      0             0  0\n","1213977  0  0      0        0     0  ...    0       0      0             0  0\n","1213978  0  0      0        0     0  ...    0       0      0             0  0\n","\n","[5 rows x 2699 columns]"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"90WsOgYiuE22","colab_type":"code","colab":{}},"source":["#df2017=df2017.drop(columns=['words'],axis=1)\n","#df2017.columns\n","df2017.to_csv(my_folder+'loans_2017_v2.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wr6NT8sztL1C","colab_type":"code","colab":{}},"source":["#https://www.w3resource.com/python-exercises/string/python-data-type-string-exercise-12.php\n","\n","def word_count(str):\n","    counts = dict()\n","    words = str.split()\n","\n","    for word in words:\n","        if word in counts:\n","            counts[word] += 1\n","        else:\n","            counts[word] = 1\n","\n","    return count\n","  \n","word_count('the quick brown fox jumps over the lazy dog.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xM-lVKdKxGzt","colab_type":"code","colab":{}},"source":["#https://stackoverflow.com/questions/46786211/counting-the-frequency-of-words-in-a-pandas-data-frame\n","#df.ACTIVITY_NAME.str.split(expand=True).stack().value_counts().to_excel(my_folder+'avy_name_wordfreq.xls')\n","df.LOAN_USE.str.split(expand=True).stack().value_counts().to_excel(my_folder+'loan_use_wordfreq.xlsx')\n","df.DESCRIPTION.str.split(expand=True).stack().value_counts().to_excel(my_folder+'desc_wordfreq.xlsx')\n","df.DESCRIPTION_NEW.str.split(expand=True).stack().value_counts().to_excel(my_folder+'desc_new_wordfreq.xlsx')\n","df.DESCRIPTION_TRANSLATED.str.split(expand=True).stack().value_counts().to_excel(my_folder+'desc_trans_wordfreq.xlsx')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dctRcIpKkOZL","colab_type":"text"},"source":["## run the code below will need very high RAM"]},{"cell_type":"code","metadata":{"id":"egAjfrxpOY_Q","colab_type":"code","colab":{}},"source":["#from IPython.display import display, HTML\n","#RAM consuming so cannot use this, try explore an option from nltk, or remove stopwords first\n","df.DESCRIPTION_TRANSLATED.str.split(expand=True).stack().value_counts().to_frame().reset_index()#.\\\n","#to_csv('/content/drive/My Drive/kiva data/check description field from loans file.csv')\n","#HTML(df3Desc.to_html())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Okx_sFXQOY-5","colab_type":"code","colab":{}},"source":["#MAY WANT TO CONSIDER THE FOLLOWING TOO:borrower_pictured,borrower_genders, town_name\n","df[['STATUS','ACTIVITY_NAME','SECTOR_NAME','COUNTRY_NAME']]=\\\n","df[['STATUS','ACTIVITY_NAME','SECTOR_NAME','COUNTRY_NAME']].astype('category')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K0CPS2QPGXPI","colab_type":"text"},"source":["### Text preprocessing using dask\n","\n","https://extrapolations.dev/blog/2015/07/reproduceit-reddit-word-count-dask/"]},{"cell_type":"code","metadata":{"id":"QeIZkE8FGmh0","colab_type":"code","colab":{}},"source":["import re\n","import time\n","import nltk\n","import dask\n","import dask.bag as db\n","import nltk\n","from nltk.corpus import stopwords"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UoqsJn6dGvrY","colab_type":"code","colab":{}},"source":["nltk.download('stopwords')\n","nltk.download('punkt')  #to use word_tokenize\n","nltk.download('wordnet')  #to use WordNetLemmatizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRRONjwTGV6n","colab_type":"code","colab":{}},"source":["no_stopwords = lambda x: x not in stopwords.words('english')\n","is_word = lambda x: re.search(\"^[0-9a-zA-Z]+$\", x) is not None"],"execution_count":0,"outputs":[]}]}